# -*- coding: utf-8 -*-
"""vlm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v9NSXInblxTvzt_DhUIqsd6kmuHWrcLn
"""

# =========================
# INSTALL
# =========================
!pip install ultralytics transformers sentence-transformers opencv-python pillow matplotlib --quiet

# =========================
# IMPORTS
# =========================
import cv2, torch, math, re
import matplotlib.pyplot as plt
from ultralytics import YOLO
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from google.colab import files

# =========================
# DEVICE
# =========================
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# =========================
# UPLOAD YOLO MODEL
# =========================
print("Upload YOLO .pt model")
model_file = files.upload()
model_path = list(model_file.keys())[0]
model = YOLO(model_path)
print("Model loaded")

# =========================
# LOAD LLM
# =========================
tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-base")
llm = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-base").to(device)

# =========================
# INTENT KEYWORDS
# =========================
INTENT_KEYWORDS = {
    "drink":["drink","drinking","water","thirst","thirsty","hydrate","sip","bottle","liquid"],
    "write":["write","writing","note","notes","exam","paper","assignment","homework","record","fill","sign"],
    "draw":["draw","drawing","sketch","sketching","outline","doodle","illustrate","diagram"],
    "paint":["paint","painting","color","colour","art","design","decorate","fill color","shade"],
    "erase":["erase","eraser","remove","delete","rub","correct","clear"],
    "sharpen":["sharpen","sharp","point","edge","tip"],
    "listen":["listen","listening","music","audio","song","sound","earphone","earphones","earpod","headphone"],
    "play":["play","playing","game","fun","entertain","activity"],
    "throw":["throw","toss","pass","hit","kick"],
    "bounce":["bounce","dribble","drop"],
    "calculate":["calculate","calculation","math","solve","count","compute","add","subtract","multiply","divide"],
    "study":["study","studying","learn","learning","revision","practice","prepare","exam","test"],
    "unlock":["unlock","open","lock","secure","safety","key"],
    "wear":["wear","wearing","put on","glasses","spectacles","vision"],
    "store":["store","keep","hold","carry","save","wallet","bag"]
}

# =========================
# INTENT â†’ AFFORDANCES (ORDERED LISTS)
# =========================
INTENT_AFFORDANCES = {
    "drink":["bottle","cup","glass"],
    "write":["pen","pencil","marker"],
    "draw":["paintbrushes","red inkpot","green inkpot","blue inkpot","pen","pencil","eraser"],
    "paint":["paintbrushes","red inkpot","green inkpot","blue inkpot","pen","pencil"],
    "erase":["eraser"],
    "sharpen":["sharpner","pencil"],
    "listen":["earpods","headphones","speaker"],
    "play":["ball","cube","toy"],
    "throw":["ball"],
    "bounce":["ball"],
    "calculate":["calculator"],
    "study":["book","notebook","pen","pencil","calculator"],
    "unlock":["keys","lock"],
    "wear":["spectacles","watch"],
    "store":["bag","wallet","bottle"]
}

# =========================
# HELPERS
# =========================
def normalize(t):
    return re.sub(r"[^a-z ]","",t.lower())

def detect_intent(t):
    for i,kws in INTENT_KEYWORDS.items():
        if any(k in t for k in kws):
            return i
    return None

def dist(a,b):
    return math.dist(a,b)

def resolve_reference(text, context):
    if any(w in text for w in ["it","that","this","one"]):
        return context["last_object"]
    return None

def direct_object_match(text, objects):
    for o in objects:
        if o["label"] in text or o["name"] in text:
            return o
    return None

# =========================
# UPLOAD IMAGE
# =========================
print("Upload image")
img_file = files.upload()
img_path = list(img_file.keys())[0]
image = cv2.imread(img_path)

# =========================
# YOLO DETECTION
# =========================
results = model(image, conf=0.3)

objects=[]
for i,b in enumerate(results[0].boxes):
    cls=int(b.cls)
    label=model.names[cls]
    x1,y1,x2,y2=map(int,b.xyxy[0])
    cx,cy=(x1+x2)//2,(y1+y2)//2
    objects.append({
        "id": i,
        "name": f"{label}_{i}",
        "label": label,
        "bbox": (x1,y1,x2,y2),
        "center": (cx,cy)
    })

# =========================
# SHOW ALL OBJECTS (X SORT)
# =========================
objects = sorted(objects, key=lambda o:o["center"][0])
print("\nDetected objects (left â†’ right):")
for i,o in enumerate(objects,1):
    print(f"{i}. {o['name']} {o['center']}")

# =========================
# DRAW
# =========================
def draw(selected=None):
    im=image.copy()
    for o in objects:
        x1,y1,x2,y2=o["bbox"]
        c=(0,0,255) if selected==o["name"] else (0,255,0)
        cv2.rectangle(im,(x1,y1),(x2,y2),c,2)
        cv2.putText(im,o["name"],(x1,y1-8),
                    cv2.FONT_HERSHEY_SIMPLEX,0.6,(255,255,255),2)
    plt.figure(figsize=(8,6))
    plt.imshow(cv2.cvtColor(im,cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.show()

draw()

# =========================
# CONTEXT MEMORY
# =========================
context={"last_object":None}

def spatial_filter(objs, ref, direction):
    rx,ry=ref["center"]
    out=[]
    for o in objs:
        ox,oy=o["center"]
        if direction=="left" and ox<rx: out.append(o)
        if direction=="right" and ox>rx: out.append(o)
        if direction=="above" and oy<ry: out.append(o)
        if direction=="below" and oy>ry: out.append(o)
    return sorted(out,key=lambda o:dist(o["center"],ref["center"]))

def choose(objs):
    if len(objs)==1:
        return objs[0]
    print("\nObjects (left â†’ right):")
    for i,o in enumerate(objs,1):
        print(f"{i}. {o['name']} {o['center']}")
    return objs[int(input("Choose number: "))-1]

# =========================
# MAIN LOOP
# =========================
print("\nAGENT READY (exit to stop)")
while True:
    user=input("\nYou: ")
    if user.lower()=="exit":
        break

    q=normalize(user)

    # ðŸ”¥ DIRECT OBJECT PRIORITY
    direct=direct_object_match(q,objects)
    if direct:
        context["last_object"]=direct
        print("Selected:",direct["name"])
        draw(direct["name"])
        continue

    intent=detect_intent(q)
    direction=next((d for d in ["left","right","above","below"] if d in q),None)
    ref=resolve_reference(q,context)

    candidates=objects

    # INTENT FILTER (RELEVANT + ORDERED)
    if intent:
        order=INTENT_AFFORDANCES[intent]
        candidates=[o for o in objects if o["label"] in order]
        candidates.sort(key=lambda o: order.index(o["label"]))

    # SPATIAL FILTER
    if direction and ref:
        candidates=spatial_filter(candidates,ref,direction)

    if not candidates:
        print("Agent: No relevant object found")
        continue

    selected=choose(candidates)
    context["last_object"]=selected
    print("Selected:",selected["name"])
    draw(selected["name"])

    out=llm.generate(
        **tokenizer(user,return_tensors="pt").to(device),
        max_new_tokens=40
    )
    print("Agent:",tokenizer.decode(out[0],skip_special_tokens=True))